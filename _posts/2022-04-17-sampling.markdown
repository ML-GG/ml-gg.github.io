---
layout: post
title:  Sampling
date:   2022-04-17 18:26:10 +0200
description:    What does it mean to sample from a distribution?
categories: posts
---

## Premise

The operation of sampling is ubiquitous is statistics and machine learning. Many algorithms often require us to sample from normal distribution, or from a posterior distribution, or sometimes even sample from a function for which the formula is not fully known.  

In this short blog-post we will explain what sampling is about and give a few examples. However, we will assume one thing, namely that **we always can sample from uniform distribution between** $$0$$ **and** $$1$$. This is a fair assumption because of pseudorandom number generators such as mt19937 and many others that have been in use for decades for this exact purpose.  

We can now move to more interesting cases.


## Sampling from normal distribution - Box-Muller

First we will learn how to effectively sample from the normal distribution, arguably the most important of all distributions. You can do it in multiple ways, but here we will present a very efficient and clever procedure that is super easy to implement, it's called Box-Muller transform. The idea is simple, you take two samples from random uniform $$\mathcal{U} (0, 1)$$, you plug them into the Box-Muller transform and in return you get two samples from standard normal distribution $$\mathcal{N}(0, 1)$$.

So how exactly do you get normal samples from the uniform ones? The idea of Box-Muller transform is the following: consider two independent random uniform variables $$U_1, U_2 \sim \mathcal{U} (0, 1)$$. If you now calculate $$Z_1 = \sqrt{-2 \ln U_1} \cos(2 \pi U_2), Z_2 = \sqrt{-2 \ln U_1} \sin(2 \pi U_2)$$, then $$Z_1, Z_2 \sim \mathcal{N}(0, 1)$$ and they are also independent. 

We can of course turn these samples from standard normal distribution (mean $$0$$ and variance $$1$$) into any arbitrary normal distribution (with mean $$\mu$$ and variance $$\sigma^2$$) simply by multiplying all samples by $$\sigma$$ and adding $$\mu$$. This is because $$ \sigma \cdot \mathcal{N} (0, 1) + \mu = \mathcal{N} (\mu, \sigma^2)$$.  


<video src="/assets/videos/sampling/BoxMuller.mp4" controls="controls" width="auto" width="100%"></video>

## Sampling from a distribution with a known formula

Can we also turn samples from random uniform into samples from other distributions such as exponential, beta, gamma, etc.? It turns out we can. To explain how let us introduce cumulative distribution function (CDF).  

Every distribution has a probability density function (PDF), a non-negative function $$f$$ that integrates to $$1$$ and describes the distribution. A cumulative distribution function (CDF) is defined as the integral of PDF: $$F(x) = \int_{-\infty}^{x} f(t) \mathrm{d} t$$. CDF has a few useful features, namely:  
* \\(\lim_{x \to - \infty} F(x) = 0\\)
* \\(\lim_{x \to + \infty} F(x) = 1\\)
* \\(F\\) is non-decreasing  

How can we use that to sample? Since the values of CDF are in range \\( [0, 1] \\) and we have access to random samples from \\( [0, 1] \\) interval, we can use the **inverse of CDF**. The concept of inverse functions is simple: if a function \\( f \\) is such that \\( f(a) = b \\), then the inverse of \\( f \\), let's call it \\( g \\), satisfies \\( g(b) = a \\). There are many inverse functions in use, for example logarithm is the inverse of exponent, n-th root is the inverse of n-th power, and many others.  

This idea works well if we know the exact formula for the inverse of CDF. For some distributions we do (eg. exponential) and for others we can approximate it (eg. normal).

<video src="/assets/videos/sampling/UniformToNormal.mp4" controls="controls" width="auto" width="100%"></video>

## What if the formula is not fully known?

There are cases in which we don't know the CDF or its inverse, better yet, we might not even fully known the formula for PDF. By not fully known we mean that \\( p(x) = \frac{1}{Z} \tilde{p} (x) \\) and we don't know the value for \\( Z \\). In that case, can we sample from \\( p \\) without having to calculate \\( Z \\) first? It turns out we can and it's very useful certain scenarios.  

Firstly, let's explain what is the deal. Consider a function \\( f(x) = \frac{1}{Z} e^{-x^2} \\). In that case \\( e^{-x^2} \\) is not a probability density function, because it doesn't integrate to \\( 1 \\). For it to be a PDF it should be scaled by the constant \\( Z \\). But it turns out we don't have to bother, we can use that \\( e^{-x^2} \\) part, ignore the \\( \frac{1}{Z} \\) part, and in result still get correct samples from \\( f \\).  

This scenario might seem artificial, but it is not. In Bayesian statistics, the posterior distribution is equal to prior distribution times likelihood, divided by a scaling constant. Formally: \\( p( \theta \| X) = \frac{p (X \| \theta) p ( \theta)}{ \int p(X \| \theta ) p(\theta) \mathrm{d} \theta } \\). The integral in the denominator is just a single number, but it's notoriously hard to calculate in some cases. If we want to sample from the posterior it would be very convenient if we could just disregard this integral and still get the samples. Luckily, there are methods to achieve that, for example **rejections sampling** or **Markov Chain Monte Carlo**.

## Rejection sampling

Rejection sampling tells us to construct a simpler **proposal distribution** \\( q(x) \\) that we can easily sample from. This might be a uniform distribution, normal distribution or whichever distribution we want, as long as we can sample from it easily. Next, we introduce a constant \\( k \\) such that \\( k q(x) \ge \tilde{p} (x) \\) for each \\( x\\). In other words we want the graph of the proposal distribution to always be above the graph of \\( \tilde{p} \\). The reason for such gymnastics will become clear shortly.  

Now, at each step of rejection sampling we generate a sample from \\( q(x) \\), let's call the sample \\( x_o \\). Remember, that we specifically requested that \\( q \\) be easy to sample from, so no issues so far. Next, we generate a sample from the uniform distribution over the interval \\( [0, kq(x_0)]\\), call that number \\( u_0 \\). Finally, we accept the sample \\( x_0 \\) if \\( u_0 < \tilde{p} (x_0) \\), otherwise we reject it. After multiple such steps the accepted samples represent samples from the distribution \\( p \\).  

Notice that we never used the value of \\( Z \\). We only sampled from \\( q \\), we sampled from a uniform distribution and we used the value of \\( \tilde{p} (x) \\), all of which we assumed to be doable.  

Rejection sampling requires us to choose the proposal distribution \\( q \\) and the constant \\( k \\). .........


## Metropolis algorithm

<video src="/assets/videos/sampling/Metropolis.mp4" controls="controls" width="auto" width="100%"></video>